---
title: "Running Inference"
---

Once your fine-tuned model is deployed, use the OpenPipe SDK to get inference from it.

First, make sure you've set up the SDK properly. See the [OpenPipe SDK](/getting-started/openpipe-sdk) section for more details. Once you've installed the SDK and set the right
`OPENPIPE_API_KEY` in your environment variables, you're almost done.

The last step is to update the model name that you're querying to match the ID of your new fine-tuned model.

<Tabs>
  <Tab title="Python">

```python
completion = openai.ChatCompletion.create(
    # model="gpt-3.5-turbo", - original model
    model="openpipe:your-fine-tuned-model-id",
    messages=[{"role": "system", "content": "count to 10"}],
    openpipe={"tags": {"prompt_id": "counting", "any_key": "any_value"}},
)
```

  </Tab>
  <Tab title="NodeJS">

```typescript
const completion = await openai.chat.completions.create({
  messages: [{ role: "user", content: "Count to 10" }],
  // model: "gpt-3.5-turbo", - original model
  model: "openpipe:your-fine-tuned-model-id",
});
```

  </Tab>
</Tabs>
